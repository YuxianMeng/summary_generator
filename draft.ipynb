{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'One of David Cameron\\'s closest friends and Conservative allies, '\n",
    "    'George Osborne rose rapidly after becoming MP for Tatton in 2001.',\n",
    "\n",
    "    'Michael Howard promoted him from shadow chief secretary to the '\n",
    "    'Treasury to shadow chancellor in May 2005, at the age of 34.',\n",
    "\n",
    "    'Mr Osborne took a key role in the election campaign and has been at '\n",
    "    'the forefront of the debate on how to deal with the recession and '\n",
    "    'the UK\\'s spending deficit.',\n",
    "\n",
    "    'Even before Mr Cameron became leader the two were being likened to '\n",
    "    'Labour\\'s Blair/Brown duo. The two have emulated them by becoming '\n",
    "    'prime minister and chancellor, but will want to avoid the spats.',\n",
    "\n",
    "    'Before entering Parliament, he was a special adviser in the '\n",
    "    'agriculture department when the Tories were in government and later '\n",
    "    'served as political secretary to William Hague.',\n",
    "\n",
    "    'The BBC understands that as chancellor, Mr Osborne, along with the '\n",
    "    'Treasury will retain responsibility for overseeing banks and '\n",
    "    'financial regulation.',\n",
    "\n",
    "    'Mr Osborne said the coalition government was planning to change the '\n",
    "    'tax system \\\"to make it fairer for people on low and middle '\n",
    "    'incomes\\\", and undertake \\\"long-term structural reform\\\" of the '\n",
    "    'banking sector, education and the welfare state.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Can not download list ot TLDs. (URLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777))\n",
      "Could not update file, using old version of TLDs list. (/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/.urlextract_tlds)\n"
     ]
    }
   ],
   "source": [
    "# encoding: utf-8\n",
    "\"\"\"\n",
    "@author: Yuxian Meng\n",
    "@contact: yuxian_meng@shannonai.com\n",
    "@version: 1.0\n",
    "@file: summarizer.py\n",
    "@time: 2018/8/27 11:17\n",
    "    LexRank Summarizer\n",
    "    Ref:\n",
    "    LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from algorithms.power_method import stationary_distribution\n",
    "from utils.text import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexRank:\n",
    "    def __init__(self, documents, stopwords=None, keep_numbers=False, keep_emails=False,\n",
    "                   keep_urls=False, include_new_words=True, ):\n",
    "        \"\"\"\n",
    "        :param documents: list:[str]\n",
    "        :param stopwords:\n",
    "        :param include_new_words:\n",
    "        \"\"\"\n",
    "        if stopwords is None:\n",
    "            self.stopwords = set()\n",
    "        else:\n",
    "            self.stopwords = stopwords\n",
    "\n",
    "        self.keep_numbers = keep_numbers\n",
    "        self.keep_emails = keep_emails\n",
    "        self.keep_urls = keep_urls\n",
    "        self.include_new_words = include_new_words\n",
    "        self.idf_score = self._calculate_idf(documents)\n",
    "\n",
    "    def get_summary_and_keywords(self, sentences, summary_size=1, threshold=.03, fast_power_method=True,\n",
    "                      return_summary=True, return_keywords=False):\n",
    "        if not isinstance(summary_size, int) or summary_size < 1:\n",
    "            raise ValueError('\\'summary_size\\' should be a positive integer')\n",
    "        sentences_new = [self.tokenize_sentence(sentence) for sentence in sentences]  # 去停用词和分词\n",
    "\n",
    "        summary, keywords=None, None\n",
    "        # 计算关键句\n",
    "        if return_summary:\n",
    "            lex_scores = self.rank_sentences(\n",
    "                sentences_new,\n",
    "                threshold=threshold,\n",
    "                fast_power_method=fast_power_method,\n",
    "            )\n",
    "\n",
    "            sorted_ix = np.argsort(lex_scores)[::-1]\n",
    "            summary = [sentences[i] for i in sorted_ix[:summary_size]]\n",
    "            print(sorted_ix[:summary_size])\n",
    "            print(np.sort(lex_scores)[::-1])\n",
    "\n",
    "        # 计算关键词\n",
    "        if return_keywords:\n",
    "            pass\n",
    "\n",
    "        return summary, keywords\n",
    "\n",
    "    def get_summary(self, sentences, summary_size=1, threshold=.03, fast_power_method=True,):\n",
    "        summary, _ = self.get_summary_and_keywords(sentences, summary_size=summary_size, threshold=threshold,\n",
    "                                                   fast_power_method=fast_power_method,\n",
    "                                                   return_summary=True, return_keywords=False)\n",
    "        return summary\n",
    "\n",
    "    def get_keywords(self, sentences, summary_size=1, threshold=.03, fast_power_method=True,):\n",
    "        _, keywords = self.get_summary_and_keywords(sentences, summary_size=summary_size, threshold=threshold,\n",
    "                                                    fast_power_method=fast_power_method,\n",
    "                                                    return_summary=False, return_keywords=True)\n",
    "        return keywords\n",
    "\n",
    "    def rank_sentences(\n",
    "        self,\n",
    "        sentences,\n",
    "        threshold=.03,\n",
    "        fast_power_method=True,\n",
    "    ):\n",
    "        if not (\n",
    "            threshold is None or\n",
    "            isinstance(threshold, float) and 0 <= threshold < 1\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                '\\'threshold\\' should be a floating-point number '\n",
    "                'from the interval [0, 1) or None',\n",
    "            )\n",
    "\n",
    "\n",
    "        tf_scores = [Counter(s) for s in sentences]\n",
    "\n",
    "        similarity_matrix = self._calculate_similarity_matrix(tf_scores)\n",
    "\n",
    "        if threshold is None:\n",
    "            markov_matrix = self._markov_matrix(similarity_matrix)\n",
    "\n",
    "        else:\n",
    "            markov_matrix = self._markov_matrix_discrete(\n",
    "                similarity_matrix,\n",
    "                threshold=threshold,\n",
    "            )\n",
    "\n",
    "        scores = stationary_distribution(\n",
    "            markov_matrix,\n",
    "            increase_power=fast_power_method,\n",
    "            normalized=False,\n",
    "        )\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def tokenize_sentence(self, sentence):\n",
    "        tokens = tokenize(\n",
    "            sentence,\n",
    "            self.stopwords,\n",
    "            keep_numbers=self.keep_numbers,\n",
    "            keep_emails=self.keep_emails,\n",
    "            keep_urls=self.keep_urls,\n",
    "        )\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def _calculate_idf(self, documents):\n",
    "        bags_of_words = []\n",
    "\n",
    "        for doc in documents:\n",
    "            doc_words = set()\n",
    "\n",
    "            for sentence in doc:\n",
    "                words = self.tokenize_sentence(sentence)\n",
    "                doc_words.update(words)\n",
    "\n",
    "            if doc_words:\n",
    "                bags_of_words.append(doc_words)\n",
    "\n",
    "        if not bags_of_words:\n",
    "            raise ValueError('documents are not informative')\n",
    "\n",
    "        doc_number_total = len(bags_of_words)\n",
    "\n",
    "        if self.include_new_words:\n",
    "            default_value = math.log(doc_number_total + 1)\n",
    "\n",
    "        else:\n",
    "            default_value = 0\n",
    "\n",
    "        idf_score = defaultdict(lambda: default_value)\n",
    "\n",
    "        for word in set.union(*bags_of_words):\n",
    "            doc_number_word = sum(1 for bag in bags_of_words if word in bag)\n",
    "            idf_score[word] = math.log(doc_number_total / doc_number_word)\n",
    "\n",
    "        return idf_score\n",
    "\n",
    "    def _calculate_similarity_matrix(self, tf_scores):\n",
    "        length = len(tf_scores)\n",
    "\n",
    "        similarity_matrix = np.zeros([length] * 2)\n",
    "\n",
    "        for i in range(length):\n",
    "            for j in range(i, length):\n",
    "                similarity = self._idf_modified_cosine(tf_scores, i, j)\n",
    "                # TODO: 添加更多的相似度，如embedding和keyword-based\n",
    "\n",
    "                if similarity:\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "                    similarity_matrix[j, i] = similarity\n",
    "\n",
    "        return similarity_matrix\n",
    "\n",
    "    def _idf_modified_cosine(self, tf_scores, i, j):\n",
    "        if i == j:\n",
    "            return 1\n",
    "\n",
    "        tf_i, tf_j = tf_scores[i], tf_scores[j]\n",
    "        words_i, words_j = set(tf_i.keys()), set(tf_j.keys())\n",
    "\n",
    "        nominator = 0\n",
    "\n",
    "        for word in words_i & words_j:\n",
    "            idf = self.idf_score[word]\n",
    "            nominator += tf_i[word] * tf_j[word] * idf ** 2\n",
    "\n",
    "        if math.isclose(nominator, 0):\n",
    "            return 0\n",
    "\n",
    "        denominator_i, denominator_j = 0, 0\n",
    "\n",
    "        for word in words_i:\n",
    "            tfidf = tf_i[word] * self.idf_score[word]\n",
    "            denominator_i += tfidf ** 2\n",
    "\n",
    "        for word in words_j:\n",
    "            tfidf = tf_j[word] * self.idf_score[word]\n",
    "            denominator_j += tfidf ** 2\n",
    "\n",
    "        similarity = nominator / math.sqrt(denominator_i * denominator_j)\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    def _markov_matrix(self, similarity_matrix):\n",
    "        row_sum = similarity_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return similarity_matrix / row_sum\n",
    "\n",
    "    def _markov_matrix_discrete(self, similarity_matrix, threshold):\n",
    "        markov_matrix = np.zeros(similarity_matrix.shape)\n",
    "\n",
    "        for i in range(len(similarity_matrix)):\n",
    "            columns = np.where(similarity_matrix[i] > threshold)[0]\n",
    "            markov_matrix[i, columns] = 1 / len(columns)\n",
    "\n",
    "        return markov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 5]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.]\n",
      "['Mr Osborne said the coalition government was planning to change the tax system \"to make it fairer for people on low and middle incomes\", and undertake \"long-term structural reform\" of the banking sector, education and the welfare state.', 'The BBC understands that as chancellor, Mr Osborne, along with the Treasury will retain responsibility for overseeing banks and financial regulation.']\n",
      "[6]\n",
      "[ 1.  1.  1.  1.  1.  1.  1.]\n",
      "['Mr Osborne said the coalition government was planning to change the tax system \"to make it fairer for people on low and middle incomes\", and undertake \"long-term structural reform\" of the banking sector, education and the welfare state.']\n"
     ]
    }
   ],
   "source": [
    "# from path import Path\n",
    "\n",
    "documents = []\n",
    "# documents_dir = Path('/data/shannon/yuxian/data/lexrank_test/bbc/politics')\n",
    "\n",
    "# for file_path in documents_dir.files('*.txt'):\n",
    "#     with file_path.open(mode='rt', encoding='utf-8') as fp:\n",
    "#         documents.append(fp.readlines())\n",
    "documents.append(sentences)\n",
    "lxr = LexRank(documents)\n",
    "\n",
    "\n",
    "\n",
    "# get summary with classical LexRank algorithm\n",
    "summary = lxr.get_summary(sentences, summary_size=2, threshold=0.03)\n",
    "print(summary)\n",
    "#[0.99317452 0.97779086 1.00605786 1.01041131 1.00011702 0.99523021 1.01721823]\n",
    "\n",
    "# get summary with continuous LexRank\n",
    "summary_cont = lxr.get_summary(sentences, threshold=None)  # 当新闻不是很长的时候，目测continous比较稳定\n",
    "print(summary_cont)\n",
    "\n",
    "\n",
    "# # 'fast_power_method' speeds up the calculation, but requires more RAM\n",
    "# scores_cont = lxr.rank_sentences(\n",
    "#     sentences,\n",
    "#     threshold=.1,\n",
    "#     fast_power_method=False,\n",
    "# )\n",
    "# print(scores_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
